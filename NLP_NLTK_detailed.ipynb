{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the NLTK package\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading corpora: Package 'corpora' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\jp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | [Errno 2] No such file or directory: 'C:\\\\Users\\\\jp\\\\\n",
      "[nltk_data]    |     AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\dolch\\\\READ\n",
      "[nltk_data]    |     ME'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import word_tokenize and pos_tag from NLTK\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens : ['we are going to start learning NLP concepts.', 'The first concept in the NLP is tokenization.', 'data-preprocess and cleaning is very importan part of NLP']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text into sentences\n",
    "para_1 = \"we are going to start learning NLP concepts. The first concept in the NLP is tokenization.  data-preprocess and cleaning is very importan part of NLP\"\n",
    "sent_tokens = sent_tokenize(para_1)\n",
    "print( \"Sentence Tokens :\" ,sent_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize text using custom trained tokeninzer \n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "text = webtext.raw('overheard.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hite guy: So, do you have any plans for this evening?\n",
      "Asian girl: Yeah, being angry!\n",
      "White guy: Oh, that sounds good.\n",
      "\n",
      "Guy #1: So this Jack guy is basically the luckiest man in the world.\n",
      "Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\n",
      "Guy #1: No; he could totally nail those two chicks.\n",
      "\n",
      "Dad: Could you tell me where the auditorium is?\n",
      "Security guy: It's on the second floor.\n",
      "Dad: Wait, you mean it's actually in the building?\n",
      "\n",
      "Girl: But, I mean, it's not lik\n"
     ]
    }
   ],
   "source": [
    "# prinitng first 500 characters \n",
    "print(text[1:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "# Use the NLTK default tokenizer and see there is problem in tokenizing 679 sentence by default tokenizer but the customize\n",
    "# tokenizer was able to tokenize sentence correctly.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents_3 = sent_tokenize(text)\n",
    "\n",
    "print(sents_3[0])\n",
    "print(sents_3[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Words Tokens :  ['This', 'is', 'a', 'very', 'simple', 'text', 'data', 'for', 'tokenizatoin', 'example']\n",
      " POS Tokesn   :  [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('very', 'RB'), ('simple', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('tokenizatoin', 'NN'), ('example', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentence into words\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"This is a very simple text data for tokenizatoin example \"\n",
    "tokens = word_tokenize(text)\n",
    "print(\" Words Tokens : \" ,tokens)\n",
    "\n",
    "pos_tokens = pos_tag(tokens)\n",
    "print(\" POS Tokesn   : \" , pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple', 'text', 'data', 'analysis', 'data', 'mining']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "english_stops\n",
    "sentence = \"this is very simple text data analysis in the data mining\"\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "[i for i in word_tokens if i not in english_stops]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this sample text stop words removal testing'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customize way of removing stop words (noise words) from a text\n",
    "stop_words_list = ['is', 'a', 'for', 'am', 'it',  'the','....']\n",
    "\n",
    "def stop_word_remove(input_text):\n",
    "    words = input_text.split()\n",
    "    remove_stop_words = [word for word in words if word not in stop_words_list]\n",
    "    cleaned_text = \" \".join(remove_stop_words)\n",
    "    return cleaned_text\n",
    "\n",
    "stop_word_remove(\"this is a sample text for stop words removal testing\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tonight we are planning to update new address'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word standardization\n",
    "dictionary_lookup = {'2nite':'tonight',  '2day':'today', 'add':'address', 'asap':'as soon as possible', 'f2f':'face to face' } \n",
    "\n",
    "def word_standardization(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []                                   # creating the empty list\n",
    "    for word in words:                               # for loop iterating for each words in the input text\n",
    "        if word.lower() in dictionary_lookup:        # check if any words present into dictionary lookup\n",
    "            word = dictionary_lookup[word.lower()]\n",
    "        new_words.append(word)\n",
    "    new_text = \" \".join(new_words)                   # joining all the words again to create the sentence.\n",
    "#        new_words.append(word) new_text = \" \".join(new_words) \n",
    "    return(new_text)\n",
    "\n",
    "word_standardization('2nite we are planning to update new add')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tonight and today we played face to face game'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_lookup = {'2nite':'tonight',  '2day':'today', 'add':'address', 'asap':'as soon as possible', 'f2f':'face to face' }\n",
    "\n",
    "def standard_text(input_text):\n",
    "    input_Text_words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in input_Text_words:\n",
    "        if word.lower() in dictionary_lookup:        # check if any words present into dictionary lookup\n",
    "            word = dictionary_lookup[word.lower()]\n",
    "        new_words.append(word)\n",
    "    new_text = \" \".join(new_words) \n",
    "    return(new_text)\n",
    "\n",
    "standard_text(\"2nite and 2day we played f2f game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'see'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming (transformation)  --- Porter Stemmer, Lancaster Stemming algorithm\n",
    "from nltk.stem import PorterStemmer\n",
    "port_stemmer = PorterStemmer()\n",
    "port_stemmer.stem('seeing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provis'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_stemmer.stem('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'owe'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_stemmer.stem('owed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'see'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming (transformation)  --- Lancaster Stemming algorithm\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "Lanc_stemmer.stem('seeing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization :  belief\n",
      "stemming :  believ\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "#Lemmatizing - Lemma is a root word and similar to stemming. \n",
    "# Lemmatizer looks meaning of the word while stemmer looks form of the word. \n",
    "# The NLTK Lemmatization method is based on WordNetâ€™s built-in morphy function.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('Lemmatization : ', lemmatizer.lemmatize('believes'))\n",
    "print('stemming : ', Lanc_stemmer.stem('believes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the lemmatize word for 'is' and 'are' are the same not 'be' because lemmatize default method of pos is 'noun', so specifically we need to define pos='v' for verb words to get the right lemmatize word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('read', 'VB'), ('this', 'DT'), ('book', 'NN'), ('in', 'IN'), ('the', 'DT'), ('flight', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering in text data \n",
    "# Part of speech tagging using nltk\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "#input_text = \" this is a sample text for part of speech tagging\"\n",
    "#input_text = \"Please book my flight for Delhi\"\n",
    "input_text = \"I am going to read this book in the flight\"\n",
    "\n",
    "words = word_tokenize(input_text)\n",
    "print(pos_tag(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we like to watch spritual movies and going to temple during weekends',\n",
       " 'data analytics is going to change the current business strategy',\n",
       " 'value education is really need in our country to increse the maturity levels']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic modeling using Gensim (LDA - Latent Dirchlet Allocation)\n",
    "document_1 = \"we like to watch spritual movies and going to temple during weekends\"\n",
    "document_2 = \"data analytics is going to change the current business strategy\"\n",
    "document_3 = \"value education is really need in our country to increse the maturity levels\"\n",
    "\n",
    "doc_combined = [document_1, document_2, document_3]\n",
    "doc_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we',\n",
       "  'like',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'spritual',\n",
       "  'movies',\n",
       "  'and',\n",
       "  'going',\n",
       "  'to',\n",
       "  'temple',\n",
       "  'during',\n",
       "  'weekends'],\n",
       " ['data',\n",
       "  'analytics',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'change',\n",
       "  'the',\n",
       "  'current',\n",
       "  'business',\n",
       "  'strategy'],\n",
       " ['value',\n",
       "  'education',\n",
       "  'is',\n",
       "  'really',\n",
       "  'need',\n",
       "  'in',\n",
       "  'our',\n",
       "  'country',\n",
       "  'to',\n",
       "  'increse',\n",
       "  'the',\n",
       "  'maturity',\n",
       "  'levels']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document split\n",
    "doc_split = [doc.split() for doc in doc_combined]\n",
    "doc_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# importing the gensim package for topic modeling\n",
    "import gensim \n",
    "#import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t0.504611340137\n",
      "  (0, 6)\t0.38376993076\n",
      "  (0, 7)\t0.504611340137\n",
      "  (0, 9)\t0.504611340137\n",
      "  (0, 10)\t0.298031586345\n",
      "  (1, 6)\t0.342619959192\n",
      "  (1, 10)\t0.266074962541\n",
      "  (1, 1)\t0.450504072643\n",
      "  (1, 4)\t0.450504072643\n",
      "  (1, 13)\t0.450504072643\n",
      "  (1, 2)\t0.450504072643\n",
      "  (2, 10)\t0.217869413811\n",
      "  (2, 14)\t0.368884983725\n",
      "  (2, 8)\t0.368884983725\n",
      "  (2, 15)\t0.368884983725\n",
      "  (2, 0)\t0.368884983725\n",
      "  (2, 3)\t0.368884983725\n",
      "  (2, 11)\t0.368884983725\n",
      "  (2, 5)\t0.368884983725\n"
     ]
    }
   ],
   "source": [
    "# using sklearn to calculate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "test_corpus = [\" this is just a sample text\", \"Text document is in unstructure format\", \" using text mining we can get the insight\"]\n",
    "model_fit = obj.fit_transform(test_corpus)\n",
    "print(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words embedding - creating text vectors\n",
    "from gensim.models import Word2Vec\n",
    "input_text = [['data', 'analytics'], ['analytics', 'science', 'data', 'big data'],['data', 'science', 'machine', 'learning'], ['deep', 'learning']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.142071281418\n"
     ]
    }
   ],
   "source": [
    "# train the model on local corpos\n",
    "model_fit = Word2Vec(input_text, min_count=1)\n",
    "print(model_fit.similarity('data', 'science'))\n",
    "#print(model_fit('science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification using NLTK\n",
    "#import textblob\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "Class_B\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model building\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "print(model.accuracy(test_corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
